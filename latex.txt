
For T = 

Time use of the different parts of the L function: 
Making Kxg            : 0.06737923622131348 (Time quadratic in T)
Making small/tempmatrx: 0.0005161762237548828
yf term               : 0.00035309791564941406
f prior term          : 0.00012135505676269531
logdet term            : 0.0001976490020751953
X prior term          : 8.845329284667969e-05

Vectorized kernel evaluation for Kx:
we must take in a vector of every other 
given all those distances (done by some function?), we can totally take the exp vectorially without using the double for loop.
This can be FIXED FAST by scipy.spatial.distance.cdist everywhere we need to create a matrix

If we need to evaluate this properly: Make log plots of the time complexity of all these observations for different T. 





After fixing: We check how well it scales
NOTE: We must replace every covariance matrix creation, and also make it work for periodic

T = 200
Making Kxg            : 0.00036263465881347656
Making small/tempmatrx: 0.0005431175231933594
yf term               : 0.00015878677368164062
f prior term          : 0.00018596649169921875
logdet term            : 0.00023674964904785156
X prior term          : 0.00015616416931152344


T = 3000
Making Kxg            : 0.0011577606201171875
Making small/tempmatrx: 0.009917497634887695
yf term               : 0.0005154609680175781
f prior term          : 0.006674051284790039
logdet term            : 0.0001289844512939453
X prior term          : 0.004412651062011719

Making Kxg            : 0.0022466182708740234
Making small/tempmatrx: 0.009105682373046875
yf term               : 0.0005736351013183594
f prior term          : 0.0070536136627197266
logdet term            : 0.00015163421630859375
X prior term          : 0.008981704711914062

T = 2000:
We seem to be getting the same results analytically with periodic and nonperiodic
## NONPERIODIC
Making Kxg            : 0.0006499290466308594
Making small/tempmatrx: 0.004570960998535156
yf term               : 0.00026535987854003906
f prior term          : 0.0031778812408447266
logdet term            : 0.0001461505889892578
X prior term          : 0.0021686553955078125

## PERIODIC
Periodic first attempt: 
Making Kxg            : 0.3972177505493164
Making small/tempmatrx: 0.004828691482543945
yf term               : 0.0003445148468017578
f prior term          : 0.003642559051513672
logdet term            : 0.00013589859008789062
X prior term          : 0.002190828323364258

After splitting into three arrays:
Making Kxg            : 0.0015492439270019531
Making small/tempmatrx: 0.004420280456542969
yf term               : 0.00025343894958496094
f prior term          : 0.00348663330078125
logdet term            : 0.00015544891357421875
X prior term          : 0.0025467872619628906

01.05.2020
Can we justify the rationale behind the offset chooser? 
An offset of X does not affect the Kx. (A scaling will affect the Kx.)
But how in the seven hells does the offset function work then????
I think in general it should not work. 
So the only thing governing the placement is the x prior term.

02.05.2020
With T=2000, downsampling=2, the F inference is real shitty.
Fixed: Now it is good. 

Making sense of bad F inference situation: 
The sigma_n was NOT set too high to be able to infer F correctly before this was given to the X.
Something with the matrix making was not correct in the new "copy-and-fix" version! Must take more care?
Also, true f copying was shallow instead of deep.
Can

I have also added some more, possible essential neurons.
Trying downsampling too: Good.
We find F nicely given the right sigma_n.
Now give it to em and find a way to initialize the F.
But banksy boomboom heyhey heyday: 
What is the robustness for finding a good F with regards to sigma???

What is the effect of neurons that have more than one tuning peak?


NEW NICE PROBLEM:
It is flipped for some parts but not flipped for entire length!!!!!
More likely where we have a long sequence of similar values.

Long story short, this plot is real head direction data inferred based only on the spikes :)
And here are some log tuning curves inferred: 
Some caveats: - Instead of the entire estimate being upside down, we also have cases where parts of the estimate is upside down, like around 1250 here.- 
I had to scale the estimate by using numerical optimization to find the best scaling factor, like I did with the position. 
This is unexpected, since the L function stays the same under repositioning but not under rescaling. 

Mailing with Ben:
First I made the evaluation of the cost function more efficient. With T=2000, it still takes some hours to converge though. 
I spent some time making sure we got the neurons we wanted from the data preprocessing, and I added the option of increasing the bin size to get more spikes per bin (Go Poisson!). 
For this case with T=2000, we're using 4000 of the original bins, two-in-one. 
This histogram shows the distribution of spikes in the current setup:
2020-05-03-em-spike-histogram-log.png
If you look at the head direction path in the previous mail, I had to select a 4000 timebin piece (from the initial 80.000) where the path does not behave periodically.  
I've worked on being able to infer it even if it's periodic, but that will be next step. Then we can increase the bin size even more without impairing the continuity of the X path, if we want to.

But first I'll finish making the presentation and start practicing :)
A cool bonus is that I didn't have to use a fancy initialization of the X like PLDS or something. The important thing was to start inferring X based on F instead of F based on X, since we get  a pretty good F estimate this way: 
There should maybe be some theory on how to transform Poisson data to look like Normal data? 
