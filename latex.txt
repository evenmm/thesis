\subsection{15.04.20}
The value of the objective function L depends with the hyperparameter sigma\_n. But does the position of local minima around the true path also vary?

Given sigma = 0.6, we compare true path and estimate and find that the estimate has a lower p value. This estimate is flipped around. 

path 1058.4738079522733
Estimate 1002.3897741405133

What happens if we shift the placing of the estimate? Or if we shift the path? Or flip the estimate? Or flip and align with the true X?

Shifting the estimate up and down gives min value at the estimate: 
shifted by -5 : 1118.238888004169
shifted by -4 : 1070.7585427495162
shifted by -3 : 1038.0394208892214
shifted by -2 : 1017.0359855482621
shifted by -1 : 1005.5726677442016
shifted by 0 : 1002.3897741405133
shifted by 1 : 1007.1324971575576
shifted by 2 : 1020.3231269647
shifted by 3 : 1043.3376392501282
shifted by 4 : 1078.3924929922844

scaling the estimate: 
scaled by 1.1 ** -5 : 2485.9833285452078
scaled by 1.1 ** -4 : 1938.1712650892023
scaled by 1.1 ** -3 : 1519.2471886941853
scaled by 1.1 ** -2 : 1216.146747465932
scaled by 1.1 ** -1 : 1054.4233313992177
scaled by 1.1 ** 0 : 1002.3897741405133
scaled by 1.1 ** 1 : 1121.07586794378
scaled by 1.1 ** 2 : 1861.9337805872415
scaled by 1.1 ** 3 : 4002.5830860546175
scaled by 1.1 ** 4 : 8631.15748918654

Flipped path kept in the same place 1139.1409233458628
Flipped path moved to the placement of estimate 1062.0333587660612
(Now we learned that the estimate and the true X do not have exactly equal shapes. They MAY be scaled versions of each other still. 

Flipped path moved to the placement of estimate 1000.6911275431675
So the estimate has even better likelihood score when flipped the right way. We interpret this to mean that it has fallen into a local optima that has a better L value than the actual estimate for this setup of parameter and f curves. 
But it is still an affine transformation of the true path. That shape yields the best fit (under this sigma) but it has lower likelihood values for some other affine versions of it. 
Affine transformation of estimate can be done to line it up with true X:
Flipped path moved to the placement of estimate 1043.08809847666
A value better than true X, but worse than X estimate that was mirrored.


Now to try with other sigma in the likelihood function (in the data creation we actually have sigma = 0)

sigma 0.285
path -3535.13732508039
Estimate -3746.7165741981266
Flipped path moved to the placement of estimate -3585.0665772629027

sigma 1.9
path 7213.622080631035
Estimate 7216.433651711254
Flipped path moved to the placement of estimate 7211.890379103076

sigma 2.8499999999999996
path 9662.270955111982
Estimate 9672.375863996107
Flipped path moved to the placement of estimate 9662.432108591025

For low sigmas, the path is worse than estimate. 
For large sigmas, the path is better. So maybe we're lucky and fall into a good local minimum. 

\subsubsection{Change the tuning curves}
Idea: Could it be due to the symmetry of the problem?
We should have more random tuning curves for a good example that has a unique solution, now that the loglikelihood approximation has enabled us to infer the X based on f.

Ideas: 
Change to less symmetric tuning curves, lke gaussian bumps with different height and width or something.

If the true X is needed to align the estimate, then the use of this for exploratory data analysis will be to find some latent variable that we do not know what is, nor on what scale. Then that must be interpreted to set up a new experiment. 
So this tool can measure how much of the variance in a dataset can be explained by an N-dimensional latent variable. And then we can hold it up to the observed whatever, and observe if the relevant variable is what causes the activity. 

\subsection{17.04.2020}
When sigma is increased to say 10, the variations in f values are smaller than the expected variation that is due to noise, and Kx looks more like a diagonal matrix. If the f don't matter so much then the most important thing is to make X smooth and small. Therefore we get a shrinked version of the estimate. 

\subsection{18.04.2020}
Checking how the different terms of the posterior respond to shifting. 
The yf term does not care
The f prior term finds the correct spot!!
The logdet term has a peak instead of a valley when the estimate is shifted to align with the true path. 
The X prior term is just soooooo fucking small when we have a continuous path!! Like 8 or 9 compared to 4000 for others. It generally prefer smaller values of X but has a small local minima around the estimated position. Cool. 

1. Verify that f prior term finds the right alignment also for other paths.
2. Maybe f prior term can work to flip? 
Do same mewthod there! Check which terms help flip around and get it done. 

\subsection{24.04.2020}
Shifting. Works well, but may be more important to let it optimize on its own.
We can shift it 0.5 or 0.1 of the estimated correct shift to allow the f values to adjust and mauybe get a better fit.
It appears that if I choose a good sigma in the beginning it gives a good result, but if I choose a bad sigma, it will settle for a bad local minima. Maybe this is true? Importance of parameter selection.

\subsection{25.04.2020}
For standard errors, bootstrap the data.

\subsection{EM algorithm \cite{dempster1977maximum} }
Incomplete data setting. 
We have Observed variables X, Unobserved variables Z; Y denotes the complete dataset of hidden and observed variables: Y = (X,Z).
$\theta^{(t)}$ is used in the calculation of E(Z|X), the expected value of the hidden given the observed. 
THat is it. 
We want to take Max likelihood of some observations X and Z, where Z is actually unobserved. 
Then start off with some estimate of $\theta$. Compute the expected loglikelihood of the observations, conditioned on x and $\theta$: The observed X stay, but for z we input the expectation given the estimate of $\theta$. 
Then from this expression take the argmax; Repeat. 

In the TMA4300 exam, the complete data likelihood was just the unobserved one that we were interested in. Then to find the Q function we find the expected loglikelihood of the hidden variabels conditioned on the observed value and the previous lambda estimate.

''we will
learn that taking the required expectation is tricker in real applications, because one
needs to know the conditional distribution of the complete data given the missing
data. Here, they were independent.''

\subsection{27.04.2020}
Kjører EM på ekte HD-data.
80 nevroner ser ikke ut til å være nok. X er knapt innom 1 og 2, ikke resten av området. 

\subsection{10.05.2020}
Finding point estimators of the hyperparameters of the Gaussian processes after we have converged to some estimates.
Taking 
§
§
§
§
§
§§
§§§
§§§
§§§§§§§
§§§§§§§§
§§§§§§§§§
§§§§§§§§§§
§§§§§§§§§

For T = 

Time use of the different parts of the L function: 
Making Kxg            : 0.06737923622131348 (Time quadratic in T)
Making small/tempmatrx: 0.0005161762237548828
yf term               : 0.00035309791564941406
f prior term          : 0.00012135505676269531
logdet term            : 0.0001976490020751953
X prior term          : 8.845329284667969e-05

Vectorized kernel evaluation for Kx:
we must take in a vector of every other 
given all those distances (done by some function?), we can totally take the exp vectorially without using the double for loop.
This can be FIXED FAST by scipy.spatial.distance.cdist everywhere we need to create a matrix

If we need to evaluate this properly: Make log plots of the time complexity of all these observations for different T. 





After fixing: We check how well it scales
NOTE: We must replace every covariance matrix creation, and also make it work for periodic

T = 200
Making Kxg            : 0.00036263465881347656
Making small/tempmatrx: 0.0005431175231933594
yf term               : 0.00015878677368164062
f prior term          : 0.00018596649169921875
logdet term            : 0.00023674964904785156
X prior term          : 0.00015616416931152344


T = 3000
Making Kxg            : 0.0011577606201171875
Making small/tempmatrx: 0.009917497634887695
yf term               : 0.0005154609680175781
f prior term          : 0.006674051284790039
logdet term            : 0.0001289844512939453
X prior term          : 0.004412651062011719

Making Kxg            : 0.0022466182708740234
Making small/tempmatrx: 0.009105682373046875
yf term               : 0.0005736351013183594
f prior term          : 0.0070536136627197266
logdet term            : 0.00015163421630859375
X prior term          : 0.008981704711914062

T = 2000:
We seem to be getting the same results analytically with periodic and nonperiodic
## NONPERIODIC
Making Kxg            : 0.0006499290466308594
Making small/tempmatrx: 0.004570960998535156
yf term               : 0.00026535987854003906
f prior term          : 0.0031778812408447266
logdet term            : 0.0001461505889892578
X prior term          : 0.0021686553955078125

## PERIODIC
Periodic first attempt: 
Making Kxg            : 0.3972177505493164
Making small/tempmatrx: 0.004828691482543945
yf term               : 0.0003445148468017578
f prior term          : 0.003642559051513672
logdet term            : 0.00013589859008789062
X prior term          : 0.002190828323364258

After splitting into three arrays:
Making Kxg            : 0.0015492439270019531
Making small/tempmatrx: 0.004420280456542969
yf term               : 0.00025343894958496094
f prior term          : 0.00348663330078125
logdet term            : 0.00015544891357421875
X prior term          : 0.0025467872619628906

01.05.2020
Can we justify the rationale behind the offset chooser? 
An offset of X does not affect the Kx. (A scaling will affect the Kx.)
But how in the seven hells does the offset function work then????
I think in general it should not work. 
So the only thing governing the placement is the x prior term.

02.05.2020
With T=2000, downsampling=2, the F inference is real shitty.
Fixed: Now it is good. 

Making sense of bad F inference situation: 
The sigma_n was NOT set too high to be able to infer F correctly before this was given to the X.
Something with the matrix making was not correct in the new "copy-and-fix" version! Must take more care?
Also, true f copying was shallow instead of deep.
Can

I have also added some more, possible essential neurons.
Trying downsampling too: Good.
We find F nicely given the right sigma_n.
Now give it to em and find a way to initialize the F.
But banksy boomboom heyhey heyday: 
What is the robustness for finding a good F with regards to sigma???

What is the effect of neurons that have more than one tuning peak?


NEW NICE PROBLEM:
It is flipped for some parts but not flipped for entire length!!!!!
More likely where we have a long sequence of similar values.

Long story short, this plot is real head direction data inferred based only on the spikes :)
And here are some log tuning curves inferred: 
Some caveats: - Instead of the entire estimate being upside down, we also have cases where parts of the estimate is upside down, like around 1250 here.- 
I had to scale the estimate by using numerical optimization to find the best scaling factor, like I did with the position. 
This is unexpected, since the L function stays the same under repositioning but not under rescaling. 

Mailing with Ben:
First I made the evaluation of the cost function more efficient. With T=2000, it still takes some hours to converge though. 
I spent some time making sure we got the neurons we wanted from the data preprocessing, and I added the option of increasing the bin size to get more spikes per bin (Go Poisson!). 
For this case with T=2000, we're using 4000 of the original bins, two-in-one. 
This histogram shows the distribution of spikes in the current setup:
2020-05-03-em-spike-histogram-log.png
If you look at the head direction path in the previous mail, I had to select a 4000 timebin piece (from the initial 80.000) where the path does not behave periodically.  
I've worked on being able to infer it even if it's periodic, but that will be next step. Then we can increase the bin size even more without impairing the continuity of the X path, if we want to.

But first I'll finish making the presentation and start practicing :)
A cool bonus is that I didn't have to use a fancy initialization of the X like PLDS or something. The important thing was to start inferring X based on F instead of F based on X, since we get  a pretty good F estimate this way: 
There should maybe be some theory on how to transform Poisson data to look like Normal data? 

\subsection{10.05.2020}
Finding point estimators of the hyperparameters of the Gaussian processes after we have converged to some estimates.
Wu does this on logscale for the parameters.
And she does it all in one big shebang.
Thankfully with no gradient. 
hypers are [sigma_x, delta_x, sigma_f, delta_f, sigma_n]
Setting the hyperparameters to these, we did not even converge at all...

\subsection{11.05.2020}
Now we just want things to work for a simple simulated example. F AND X inference.
Which means that we're actually doing something new here. Earlier we did this given true f.
Exploring:
Initial F: Pretty good with np.sqrt(y_spikes) - np.amax(np.sqrt(y_spikes))/2
Scaling / offset together does nothing. Trying both L and just f prior.
(Also, with good conditions we don't really need offset or scaling)
How good are our F estimates? 
Much lower than the true ones. Here we have very Narrow peaks though.

Trying periodic: 
Very seasick with regards to up and down.
Only allow path to go outside (0, 2pi) if you deal with that in spike generation.

To get a good benchmark we use:
Many neurons
Much time
Good initialization
Some random path
Narrow tuning width --> NO! Wide tuning width 06.07
Strong tuning_strength 

Then, when it works, let's start with many neurons and much time and see how strong the tuning must be for us to find it.

OBSERVATAION::::
When we changed to not uniform distribution of widths and position, inference worked better!
MaY BE DUE TO just wider tunings. (Yes.)
Can check this by just varying positions while keeping widths the same.
Then task will be to check what width is optimal.

\subsection{12.05.2020}
There is a very high background noise rate. Neurons have 1 spike most of the time. 
Background noise should be set. 

\subsection{14.05.2020}
Too sharply defined tuning curves gives too sharp local minima.

# of neurons, weight on X(t), background noise, … 
length of recording 
other kinds of noise: other tuning, etc. 


\subsection{18.05.2020}
Implementing gradient

h = 1e-5
numderiv = (L(x+h) - L(x))/h 


\subsection{19.05.2020}
Using cdist function.
400 by 200 matrix: Self-defined distance function.
0.1 seconds used per operation in distance function.
Versus 0.0003 for inbuilt.

!! I assume that lambda u,v refers to u from XA and v from XB.


\subsection{20.05.2020}
Trying out the gradient I implemented.
In total: Difference is 18.81434038901393
logdet term: 6.801736937448393
f prior term: 297.3210614517232
x prior term: 3.3325630662444756e-05


T = 100
xprior: 2.5*10^-6
fprior: 69.80775303343978


TIME USE: 
I_times_dK_B_K = np.matmul(I_minus_column_tensor, dKx_B_inv_Kg)
Multiply (T x 1) x T , (1 x T) x T
0.21-22

K_B_dK_times_I = np.matmul(Kx_B_inv_dKg, I_minus_row_tensor)
Multiply (1 x T) x T , (T x 1) x T
0.20

squarebrackets = I_times_dK_B_K + K_B_dK_times_I
Addition of T TxT matrices
0.11

fM = np.matmul(F_tensor, squarebrackets)
0.12



\subsection{23.05}
I tried the stupid idea. It was fast but wrong. 
Maybe if I'm lucky I can now use the fast techniques to get the original one faster.

If sigma is small the difference between the analytical gradient and the numerical is bigger.

Exciting new speed check of revamped speedy original f prior gradient term:
Speedcheck of x_jacobian function:
Making Kxg            : 0.0001239776611328125
Making B and B inverse: 7.796287536621094e-05
Making small/tempmatrx: 0.00014638900756835938
logdet term            : 0.06461167335510254
f prior term          : 0.07611465454101562
X prior term          : 0.00014853477478027344
Speedcheck of x_jacobian function:
Making Kxg            : 0.00012040138244628906
Making B and B inverse: 7.390975952148438e-05
Making small/tempmatrx: 0.0001380443572998047
logdet term            : 0.05887794494628906
f prior term          : 0.07275986671447754
X prior term          : 0.00015592575073242188

HMMMMMM! Many iterations do not seem to help us converge....
I'd sooner say we diverge.
Hm.

\subsection[24.05.2020]
Making the perfect testing example
Voila! Wider tuning curves gives us much better performance! ! ! 
The y spikes plot has white lines. But that is just the scren resolution.
Conclusion: 
The analytical handles the offset all by itself! From this day onward we are free from the offset crutch!!
And is it stable when we really drop sigma: It converges <3 
And are the estimate values still better than path: yes. May be due to randomness, numerical error, OR: simply the fact that when we generate from the GP it will not be the max likelihood
thing in the prior sense, and this plays a part. It is a strugglebetween path correctness and f generative correctness. The path was created just from GP simulation. 
Therefore it is not optimal!! Even if we have true f given, it is not the randomness but sooner the fact that it is drawn from a GP and is therefore strange. Then f helps us kinda find it anyway. 

Starting with very low sigma: 

Now: What tuning width is optimal? 
How about periodic? 
Can we run this on real HD data too? 


\subsection{27.05}
Robustness plots
* Tuning strength *
Bumptuningfunction defines true f values. These can be negative.
Best to define tuning strength as f value at peak! 
We have defined baseline f value with no tuning to be -10 (avoids background noise)

T = 100
lambda  rms
0.1     0.5165187057084998
0.2     0.5194637238709657
 1      0.41777308117293027
 1 flip 0.41415404503400055 (Flipped after convergence)
 2      0.2356399579694827
 3      
 4 
 8 0.5201834760253742
 9 
10 
11 
12 0.09598990632141391
16 0.6762482397087992
T = 1000

T = 10.000



\subsection{28.05}
Making efficient script.

When we are unable to infer the X due to too low tuning strength, and end up with a flat line as estimate: then the RMS value will only depend on the variance of X. 
The R squared value would maybe be better, but if we average the RMS over seeds then we average over the variance of X path as well.

If X estimate is precisely zero we get an error in the determinant of Kx.
That is fixed by adding a small term.
But that problem could also be due to the constant dropping of sigma n without restoring it!
Needs a global unchangable startign sigma n and then we decrease it locally at every iteration.

Had to reset F estimate when we flip X estimate because I think it was trapped in a local minima leading to worse convergence for X

Re-converging after flipping gave worse RMSE values, so we're just taking the min RMSE vlaue of X and X flipped.
Though could it be the case that X converges better if it is flipped?
-- Now for ts 16 the after flip is always better

\subsection{29.05}
Discovered that: ts 16, good converge except for line stuck far down at end.
Attempts at fixing it: 
- re-adding random noise for initial guess:
Worked for that case.
No-flip RMSE: 0.3764127947575389

We are not spreading across enough of the domain.
- Try lowering the sigma_x from 100 to 50:
Did not work
- Try increasing the variance of generative (and inferring) X
Worked (obviously)

Having convergence issues at seed 9:
- that is right after plot figures warning.
- it is also a seed that gave us a very low variation in X except for a small number of sharp peaks.

######################################
Lambda strength: 16
Baseline expected spikes: 4.5399929762484854e-05
Expected no. of spikes at peak: 16
RMSE for X, Averaged across seeds: 0.2587602343114252
RMSE for X flipped: 1.221666999270192
RMSE for F, Averaged across seeds: 8.422289793214146
######################################

T = 100
lambda_strength and RMSE
16: 0.43703416548029617
12: 0.49388507368447404
10: 0.45196784300534765
8: 0.3528263031609709
6: 0.3867000231202763
4: 0.45159691185191636
2: 0.739551482437652
1: 

ts 2 has some unfortunate cases where it is upside down only in parts
Unfortunate seeds: 2, 14, (4 is unfortunate for 12), 

T = 10
lambda_strength and RMSE and R squared
Averaged over 12 seeds 
0.1 1.0364895956799882  -50.4728346460829
0.5 1.0406274710325414  -50.47464357537558
1   0.7422087009697954  -20.3087918773691
2   0.7466309328194045  -20.328251915084937
3   0.7472190837927086  -20.328615752135146
4   0.7527862724434419  -20.35709780525213
5   0.7404025520105525  -20.321719480758173
6   0.7350274848878883  -20.304027117377405
7   0.7760717833525844  -20.661516639707507
8   0.746736816844552   -20.409954797527746
9   0.7642916964361809  -20.61791412042584
10  0.7342163509093863  -20.44656243089726
11  0.6783056187618514  -20.138536419227183
12  0.6734560913326089  -20.129929968875064
13  0.6679172321859862  -20.115018289674417
14  0.6534704062828897  -20.081188426730588
15  0.6512345921874434  -20.238918624651173
16  0.6354140567049367  -20.044956895538427

T = 100

T = 1000
Ganske bra rmse for T = 1000! 
Kan godt kjøre en plotterunde.
Men vi trenger altså flere lave tuning strengths 


\subsection{02.05}
OpenMP driver og parallelliserer numpy-koden for å få brukt mer juice når jeg kjører ett skript (noe jeg har observert på min maskin).
Det mest effektive som Per Kristian foreslår er å begrense kjernebruken til 1 CPU per prosess, så bruken av hver CPU blir optimal, og så se hvor mange prosesser vi har plass til på siden av hverandre. 
Plan 1: 
Alternativ: Ikke begrens antall CPU-er i Open MP og kjør færre prosesser på siden av hverandre. 

\subsection{03.05}
export OMP_NUM_THREADS=1 setter 1 CPU på hver parallelle tråd. Antall tråder = len(seed).

Noticed difference in performance.
There's a fucking bug that makes the results bad. May come from adding function arguments to shit. 
Relax, we have different versions. We can handle this. 
- Attempt to highlight changes and see what is being done differently:

Checking how important initial X is:
L value for path varies due to different F!!!!!: 6287.955163043913, , 
seed 0       , offset 1.5: L -13565.381001321895 
seed original, offset 1.5: L -14806.454455027344    path                            sqrt(mean(X_estimate-previous_X))= 0.0017169788566815862
seed 2       , offset 1.5: L -13878.157314977567    path                            sqrt(mean(X_estimate-previous_X))= 0.0037116017844465625
seed 2       , offset 1.0: L -14801.72865808279     path                            sqrt(mean(X_estimate-previous_X))= 0.008494824335477224
seed 2       , offset 6.0: L  6249.93726898615      path  6287.955163043913         sqrt(mean(X_estimate-previous_X))= 0.04860226552057797 (This one flipped)
seed 2       , offset 5.0: L -13876.44994915217     path  -9755.027979408747        sqrt(mean(X_estimate-previous_X))= 0.048404832338805145 
seed 2       , offset 4.0: L -13876.44994915217     path  -17883.73500880844        sqrt(mean(X_estimate-previous_X))= 0.007112102245324013 
seed 0       , offset 4.0: L -15419.344845948024     path  -8550.152602300686        sqrt(mean(X_estimate-previous_X))= 0.002785984840746242 
frisluppen,    2pi random: L -14497.101410959049     path  -8550.152602300686        sqrt(mean(X_estimate-previous_X))= 0.007158166109343249 (FLIPPER IKKE!)
helt random start, path seed 12 i stedet for 11  DNF (Did not flip)
path seed 13 DNF
14 

Conclusion: Enough for a discussion, possibly a plot showing different results and having different L values given true F.


Changed on robust-efficient-script to include extra parameters. RMSE difference 0.08330439712884885 which is almost nothing. 
Must be something else. 

I suspect it may be due to the spread of tuning curves outside range of X when path does not spread.
For seed 16 path spread, ok inference.
Seed 

For good performance: 
Let the inducing grid stay in the range of the path! Huzzah! That is realistic anyway <3 
Actually quite important since we have to talk with the inducing points that are outside and also lose precision.

Now to decide whether to initialize X over the range of the entire X or just in the width of the path? Can do it in 0,2pi because we can. 
Even though we say that we know the X when we place the inducing points there. Those can be moved though. But is job.

############ Today's changes to parallel:
- change initialization to 2pi random random
- make x_grid_inducing range only over the path of X
- make x_grid_inducing a variable passed to function
- Kt still proiduced outside, x_grid_inducing and Kggplain inside.

Original:
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=16
with result:
We are using 16 cores
We are using 16 cores per node
Total of 16 cores
Time use: 5.85500431060791
Time use: 2.7465834617614746
Time use: 4.408917665481567
or 
    #SBATCH --nodes=1
    #SBATCH --ntasks-per-node=20
with result:
#SBATCH --nodes=1
with result:
We are using 20 cores
We are using 20 cores per node
Total of 20 cores
Time use: 2.641716718673706
Time use: 2.4316775798797607
Time use: 4.215567350387573

Alternative: 
    #SBATCH --ntasks=20
with result
We are using 20 cores
We are using 20 cores per node
Total of 20 cores
Time use: 5.856815814971924
Time use: 2.4280481338500977
Time use: 4.860387563705444

Much of the same. 

I am calling 20 processes, and I think this means 20 tasks. I don't care about hte number of nodes this is spread on. 

----- Time use on cluster:
T=10:
Between 2 and 101 seconds for an all seed iteration
Mean about 50 sek.
Times 25, and we're at 1250 sek = 20 min

\subsection{04.06}
Attempt at fixing flipping: 
Took estimate after 1 iteration and continued converging from there -> worse results. OK.
Or in some cases it is actually better since it escapes the local minima.
RMSe values: 
Old flipping regime: 
#############################
Seed: 5
RMSE for X: 2.000361249666058
RMSE for X flipped: 0.757352511105472
Re-iterating because of flip
RMSE for X: 0.30142939816461645
RMSE for X flipped: 0.757352511105472
L value for path: -7631.550567025655
L value for X: -10266.25078619499
Time for one iteration: 187.27693104743958
Seed: 6
RMSE for X: 2.0596136872448176
RMSE for X flipped: 0.5175562624817303
Re-iterating because of flip
RMSE for X: 0.2391187534574545
RMSE for X flipped: 0.5175562624817303
L value for path: 6554.17805983942
L value for X: 6508.124605750462
Time for one iteration: 23.47577929496765
Seed: 8
RMSE for X: 0.4180241777651441
RMSE for X flipped: 0.26133791003888834
L value for path: 6482.07908704072
L value for X: 6414.940573150899
L value for previous X: 8541.064856381898
Time for one iteration: 154.43976879119873


New flipping regime: 
##############################
Lambda strength: 8
Seed: 5
RMSE for X: 2.000361249666058
RMSE for X with flipstart: 0.7621501051176602
Comparing L values, setting F_estimate to true F
L value for path: 8005.422829962662
L value for X: 13883.96852137041
L value for X with flipstart: 13810.3239693414
Time for one iteration: 63.12100887298584
Seed: 6
RMSE for X: 2.0596136872448176
RMSE for X with flipstart: 0.24589981784721887
Comparing L values, setting F_estimate to true F
L value for path: 7833.596953810112
L value for X: 7616.131392448836
L value for X with flipstart: 7631.840703670283
Time for one iteration: 42.692402362823486
Seed: 8
RMSE for X: 3.005939845783482
RMSE for X with flipstart: 0.2741920654942813
Comparing L values, setting F_estimate to true F
L value for path: -2087.97177590144
L value for X: -34.143720114132364
L value for X with flipstart: -83.78201820775726
Time for one iteration: 282.0005621910095

Averaged over these three, the older version seems better in RMSE.



See the abomination that is partly upside down? This can happen. Fortunately, the ones with better RMSE usually have better loglikelihood scores as well, so we can just choose the estimate with the best loglikelihood value.
rmse from indexing: 
[0.1436712  0.14436951 0.14432622 0.14840053 0.12341689 0.14436426
 0.15332464 0.14737077 0.1146582  1.26986762 0.11476183 0.14436408
 0.14664441 0.14432997 0.1451634  0.14395113 0.1397909  0.14981116
 0.1233547  0.14439709]
L values with corresponding F estimate 
[ 5319.52987402  6519.46143327  6519.46304017  6519.46352601
  5019.6842911   6519.46165313 -9950.97827048  6519.46008073
  4120.0972603   6711.77682715  4120.09504089  6519.4607539
  6519.47156508  6519.46088204  6519.5128145   6219.30029929
  6219.30529595  6519.46691403  5019.71796545  6519.46055077]
L values with true F 
[ 8096.60180873  9158.52973905  9158.51221731  9158.24053934
  7827.97982071  9158.34971051   547.42634028  9158.19965167
  7015.41976746 10522.36920996  7015.4253867   9158.34782001
  9159.05330574  9158.43887206  9157.05285068  8891.30554163
  8891.4643102   9158.24812486  7828.30491924  9158.54737181]

Very bad results when x_grid_induce follows the estimate.

From mail: 
The covariance function of a stationary process can be represented as theFourier transform of a positive finite measure.
In the case that the spectral density S(s) exists, the covariance function andthe spectral density are Fourier duals of each other as shown in eq. (4.6).
http://www.gaussianprocess.org/gpml/chapters/RW.pdf pg 82.

\subsection{06.06}
The inference is super bad for T=1000 (!)
Initial positioning may have more significance for higher T??
Y E S .
2pirandom starts horrible for high T.


\subsection{07.06}

HOW TO PLOT: 
if T == 1000:
    plt.figure(figsize=(10,3))
else:
    plt.figure()
plt.figure()
plt.title("Final estimates") 
plt.xlabel("Time")
plt.ylabel("x")
plt.plot(path, color="black", label='True X')
plt.plot(X_estimate, label='Estimate')
plt.legend(loc="upper right")
plt.tight_layout()
plt.savefig(time.strftime("./plots/%Y-%m-%d")+"-ii-final.png")

\begin{figure}[H]
    \centering
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-07-ii-initial.png}
    \end{minipage}\hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \includegraphics[width=0.98\linewidth]{fig/2020-06-07-ii-final.png}
    \end{minipage}
    \caption{Left: 7 Initial starting points. The initial estimates are independent uniform samples between 0 and $2\pi$ for every timepoint $\vect{x}_t$. Right: Final estimates. }
    \label{upsidedown1}
\end{figure}

Old ensemble idea before flat start proved worthy: 
\subsection{Ensemble of starting points}
4 seeds for initial start, take best L value. 
Show ensemble of 4 seeds for good starting position.
\subsection{Hack-y solution for dealing with partly flipped estimates}
As can be seen in figure \ref{upsidedown2}, there is (usually) a bigger discontinuity than usual in \matr{X}.
\begin{itemize}
    \item Find discontinuity points.
    \item For each interval between, find best if flipped or not flipped. 
    \item Re-iterate from the edited estimate 
    \item If necessary, repeat until no discontinuities
\end{itemize}


W O O W !!
Random noise makes the start worse!!!! 
Start it just flatly!!!
(Then no ensemble :o ) 

Lærdom: 
Når jeg sender mail til bEN med ting som ikke funker, men fremdeles har ting jeg kan gjøre eller skrive opp, så kan jeg fort finne ut av det ved bare å skrive det opp systematisk
eller prøve ut ting. 
Mye artigere å sende gode resultater og ting som funker. 
Men når man lurer på ting er det bare å sende.
Spesielt fordi man får ordnet tankene sine på det viset. 


Interestingly: When started at true X, it converges to something worse than when it is started flatly!!!!! ???? !!!!!
For flat start: 0.17342200792078588
For true start: 0.2890430891684393

Attempt for cluster: 
5 different path seeds, 1 starting (1*np.ones(T)), 4 processors each to maybe speed up times 4? 
Also inrease tolerance, from 1e-5 to 1e-3, decrease N_iterations from 200, and remove Plotting

First, try 5 different seeds for T = 1000 locally to see if rmse is any good with flat start: Flat start, no problem, found 5 good seeds for path.
Then run T = 1000 on cluster: 
Run T = 10 and 100 locally at the same time: 
Old time uses T=1000, from high to low tuning strrength
8183.067131280899
8581.958387851715
9471.475401639938
9823.432631731033
9601.43127989769
9191.518896579742
9204.245648384094
8517.707441091537
5377.434408426285
9633.522401809692
5230.836555480957
7294.699887752533
5097.766581058502
6921.350073814392 4
6690.050822257996 3
3777.9227628707886 2
5069.2377343177795 1
4270.729000329971
4199.376188278198
4149.991794347763
2194.7684655189514
352.05941796302795 0.01

Check if overflow still exists?: Nope
And that's a good day for a good boy.

\subsection{08.06}
What is the bin size for these simulated experiments? Does not matter, all that matters is the expected # spikes in a bin.
Trying 5 tasks per node to see if we get 20 processors with that and OMP_NUM_THREADS = 4

We can't confine the X estimate to a fixed interval. 
So we must allow for a big safe margin on both sides instead. 
The inducing grid must be placed on this wide range, and the neurons too. 
Then we find offset after convergence and calculate rmse.
To do: 
- Rescale inducing grid, tuning curve definition etc to [0,10]
- Let path start around 5 and let it wonder truly freely. 

\subsection{09.06}
Okaaaay, I decoupled the inducing points and neuron selectivity from the actual path, and now the inference is bad.
- Tried changing the inducing grid to the right place - did not work.
- tried starting in the right region - nope
- Maybe the neurons being outside and inactive hampers it? - nope

What was the problem earlier when we had bad inference for T=1000? - fixed by initialization better
- Found bug in max neuron place definition
Also changed the mean of true path to not depend on the induce grid limits.

What have we learned today? 
Having inactive neurons outside the range of X is not dangerous.

\subsection{10.06}
Before changing from [0,2pi] and inducing grid in the right place, accuracy was good.
See effect of changes:
 - Then we must also increase the sigma for the X. Did not improve.
 --> bad result is due to placement of inducing grid
What we CAN do, is let the inducing grid be placed according to the x estimate!!

 - - Do PCA Staart!!!!! Just leave out the neurons that are not part
The scale of X is related to the variance parameter in the prior. 

----- Meeting with Ben: 
*PCA, get starting done, see if better
*Choose X or F start and write about it
----- For 11.06.2020:
Add F plot for partly upside down starts, write about it and say "sorry, we can't do better if initial is bad" :))))
"This is the type of thing that makes you scratch your head and come up with a better model"
Finish all the talk about starting and convergence and optimization problems
Start robustness plot with PCA
While running, explore boons of different ensemble startings to see if worth it
Then: 
Try out periodic, does it work quickly? 
Apply to Peyrache data as nice result
Fit nonperiodic to Peyrache data
Compare the two and make comments about ways to do model evaluation
Not obvious if we will get multiple bumps or stay inside range and suffer when fitting nonperiodic to problem with periodic geometry
(Possibly compare to Bernoulli)

PCA:
FIt X first based on F, starting from PCA: Maybe better but still has problems with partly flipping
Fit F first based on PCA X: 
- When X did not reach the entire range, it was bad
- But when it did reach the entire range, it was good

- No need for PCA length to be exactly equal to delta x

