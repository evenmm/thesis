
For T = 

Time use of the different parts of the L function: 
Making Kxg            : 0.06737923622131348 (Time quadratic in T)
Making small/tempmatrx: 0.0005161762237548828
yf term               : 0.00035309791564941406
f prior term          : 0.00012135505676269531
logdet term            : 0.0001976490020751953
X prior term          : 8.845329284667969e-05

Vectorized kernel evaluation for Kx:
we must take in a vector of every other 
given all those distances (done by some function?), we can totally take the exp vectorially without using the double for loop.
This can be FIXED FAST by scipy.spatial.distance.cdist everywhere we need to create a matrix

If we need to evaluate this properly: Make log plots of the time complexity of all these observations for different T. 





After fixing: We check how well it scales
NOTE: We must replace every covariance matrix creation, and also make it work for periodic

T = 200
Making Kxg            : 0.00036263465881347656
Making small/tempmatrx: 0.0005431175231933594
yf term               : 0.00015878677368164062
f prior term          : 0.00018596649169921875
logdet term            : 0.00023674964904785156
X prior term          : 0.00015616416931152344


T = 3000
Making Kxg            : 0.0011577606201171875
Making small/tempmatrx: 0.009917497634887695
yf term               : 0.0005154609680175781
f prior term          : 0.006674051284790039
logdet term            : 0.0001289844512939453
X prior term          : 0.004412651062011719

Making Kxg            : 0.0022466182708740234
Making small/tempmatrx: 0.009105682373046875
yf term               : 0.0005736351013183594
f prior term          : 0.0070536136627197266
logdet term            : 0.00015163421630859375
X prior term          : 0.008981704711914062

T = 2000:
We seem to be getting the same results analytically with periodic and nonperiodic
## NONPERIODIC
Making Kxg            : 0.0006499290466308594
Making small/tempmatrx: 0.004570960998535156
yf term               : 0.00026535987854003906
f prior term          : 0.0031778812408447266
logdet term            : 0.0001461505889892578
X prior term          : 0.0021686553955078125

## PERIODIC
Periodic first attempt: 
Making Kxg            : 0.3972177505493164
Making small/tempmatrx: 0.004828691482543945
yf term               : 0.0003445148468017578
f prior term          : 0.003642559051513672
logdet term            : 0.00013589859008789062
X prior term          : 0.002190828323364258

After splitting into three arrays:
Making Kxg            : 0.0015492439270019531
Making small/tempmatrx: 0.004420280456542969
yf term               : 0.00025343894958496094
f prior term          : 0.00348663330078125
logdet term            : 0.00015544891357421875
X prior term          : 0.0025467872619628906

01.05.2020
Can we justify the rationale behind the offset chooser? 
An offset of X does not affect the Kx. (A scaling will affect the Kx.)
But how in the seven hells does the offset function work then????
I think in general it should not work. 
So the only thing governing the placement is the x prior term.

02.05.2020
With T=2000, downsampling=2, the F inference is real shitty.
Fixed: Now it is good. 

Making sense of bad F inference situation: 
The sigma_n was NOT set too high to be able to infer F correctly before this was given to the X.
Something with the matrix making was not correct in the new "copy-and-fix" version! Must take more care?
Also, true f copying was shallow instead of deep.
Can

I have also added some more, possible essential neurons.
Trying downsampling too: Good.
We find F nicely given the right sigma_n.
Now give it to em and find a way to initialize the F.
But banksy boomboom heyhey heyday: 
What is the robustness for finding a good F with regards to sigma???

What is the effect of neurons that have more than one tuning peak?


NEW NICE PROBLEM:
It is flipped for some parts but not flipped for entire length!!!!!
More likely where we have a long sequence of similar values.

Long story short, this plot is real head direction data inferred based only on the spikes :)
And here are some log tuning curves inferred: 
Some caveats: - Instead of the entire estimate being upside down, we also have cases where parts of the estimate is upside down, like around 1250 here.- 
I had to scale the estimate by using numerical optimization to find the best scaling factor, like I did with the position. 
This is unexpected, since the L function stays the same under repositioning but not under rescaling. 

Mailing with Ben:
First I made the evaluation of the cost function more efficient. With T=2000, it still takes some hours to converge though. 
I spent some time making sure we got the neurons we wanted from the data preprocessing, and I added the option of increasing the bin size to get more spikes per bin (Go Poisson!). 
For this case with T=2000, we're using 4000 of the original bins, two-in-one. 
This histogram shows the distribution of spikes in the current setup:
2020-05-03-em-spike-histogram-log.png
If you look at the head direction path in the previous mail, I had to select a 4000 timebin piece (from the initial 80.000) where the path does not behave periodically.  
I've worked on being able to infer it even if it's periodic, but that will be next step. Then we can increase the bin size even more without impairing the continuity of the X path, if we want to.

But first I'll finish making the presentation and start practicing :)
A cool bonus is that I didn't have to use a fancy initialization of the X like PLDS or something. The important thing was to start inferring X based on F instead of F based on X, since we get  a pretty good F estimate this way: 
There should maybe be some theory on how to transform Poisson data to look like Normal data? 

A cool bonus is that I didn't have to use a fancy initialization of the X like PLDS or something. The important thing was to start inferring X based on F instead of F based on X, since we get  a pretty good F estimate this way: ...
There could maybe be some theory on how to transform Poisson data to look like Normal data? 


\subsection{10.05.2020}
Finding point estimators of the hyperparameters of the Gaussian processes after we have converged to some estimates.
Wu does this on logscale for the parameters.
And she does it all in one big shebang.
Thankfully with no gradient. 
hypers are [sigma_x, delta_x, sigma_f, delta_f, sigma_n]
Setting the hyperparameters to these, we did not even converge at all...

\subsection{11.05.2020}
Now we just want things to work for a simple simulated example. F AND X inference.
Which means that we're actually doing something new here. Earlier we did this given true f.
Exploring:
Initial F: Pretty good with np.sqrt(y_spikes) - np.amax(np.sqrt(y_spikes))/2
Scaling / offset together does nothing. Trying both L and just f prior.
(Also, with good conditions we don't really need offset or scaling)
How good are our F estimates? 
Much lower than the true ones. Here we have very Narrow peaks though.

Trying periodic: 
Very seasick with regards to up and down.
Only allow path to go outside (0, 2pi) if you deal with that in spike generation.

To get a good benchmark we use:
Many neurons
Much time
Good initialization
Some random path
Narrow tuning width
Strong tuning_strength 

Then, when it works, let's start with many neurons and much time and see how strong the tuning must be for us to find it.

OBSERVATAION::::
When we changed to not uniform distribution of widths and position, inference worked better!
MaY BE DUE TO just wider tunings. 
Can check this by just varying positions while keeping widths the same.
Then task will be to check what width is optimal.

\subsection{12.05.2020}
There is a very high background noise rate. Neurons have 1 spike most of the time. 
Background noise should be set. 

\subsection{14.05.2020}
Too sharply defined tuning curves gives too sharp local minima.

# of neurons, weight on X(t), background noise, … 
length of recording 
other kinds of noise: other tuning, etc. 


\subsection{18.05.2020}
Implementing gradient

h = 1e-5
numderiv = (L(x+h) - L(x))/h 


\subsection{19.05.2020}
Using cdist function.
400 by 200 matrix: Self-defined distance function.
0.1 seconds used per operation in distance function.
Versus 0.0003 for inbuilt.

!! I assume that lambda u,v refers to u from XA and v from XB.


\subsection{20.05.2020}
Trying out the gradient I implemented.
In total: Difference is 18.81434038901393
logdet term: 6.801736937448393
f prior term: 297.3210614517232
x prior term: 3.3325630662444756e-05


T = 100
xprior: 2.5*10^-6
fprior: 69.80775303343978


TIME USE: 
I_times_dK_B_K = np.matmul(I_minus_column_tensor, dKx_B_inv_Kg)
Multiply (T x 1) x T , (1 x T) x T
0.21-22

K_B_dK_times_I = np.matmul(Kx_B_inv_dKg, I_minus_row_tensor)
Multiply (1 x T) x T , (T x 1) x T
0.20

squarebrackets = I_times_dK_B_K + K_B_dK_times_I
Addition of T TxT matrices
0.11

fM = np.matmul(F_tensor, squarebrackets)
0.12



\subsection{23.05}
I tried the stupid idea. It was fast but wrong. 
Maybe if I'm lucky I can now use the fast techniques to get the original one faster.

If sigma is small the difference between the analytical gradient and the numerical is bigger.

Exciting new speed check of revamped speedy original f prior gradient term:
Speedcheck of x_jacobian function:
Making Kxg            : 0.0001239776611328125
Making B and B inverse: 7.796287536621094e-05
Making small/tempmatrx: 0.00014638900756835938
logdet term            : 0.06461167335510254
f prior term          : 0.07611465454101562
X prior term          : 0.00014853477478027344
Speedcheck of x_jacobian function:
Making Kxg            : 0.00012040138244628906
Making B and B inverse: 7.390975952148438e-05
Making small/tempmatrx: 0.0001380443572998047
logdet term            : 0.05887794494628906
f prior term          : 0.07275986671447754
X prior term          : 0.00015592575073242188

HMMMMMM! Many iterations do not seem to help us converge....
I'd sooner say we diverge.
Hm.

\subsection[24.05.2020]
Making the perfect testing example
Voila! Wider tuning curves gives us much better performance! ! ! 
The y spikes plot has white lines. But that is just the scren resolution.
Conclusion: 
The analytical handles the offset all by itself! From this day onward we are free from the offset crutch!!
And is it stable when we really drop sigma: It converges <3 
And are the estimate values still better than path: yes. May be due to randomness, numerical error, OR: simply the fact that when we generate from the GP it will not be the max likelihood
thing in the prior sense, and this plays a part. It is a strugglebetween path correctness and f generative correctness. The path was created just from GP simulation. 
Therefore it is not optimal!! Even if we have true f given, it is not the randomness but sooner the fact that it is drawn from a GP and is therefore strange. Then f helps us kinda find it anyway. 

Starting with very low sigma: 

Now: What tuning width is optimal? 
How about periodic? 
Can we run this on real HD data too? 


\subsection{27.05}
Robustness plots
* Tuning strength *
Bumptuningfunction defines true f values. These can be negative.
Best to define tuning strength as f value at peak! 
We have defined baseline f value with no tuning to be -10 (avoids background noise)

T = 100
lambda  rms
0.1     0.5165187057084998
0.2     0.5194637238709657
 1      0.41777308117293027
 1 flip 0.41415404503400055 (Flipped after convergence)
 2      0.2356399579694827
 3      
 4 
 8 0.5201834760253742
 9 
10 
11 
12 0.09598990632141391
16 0.6762482397087992
T = 1000

T = 10.000



\subsection{28.05}
Making efficient script.

When we are unable to infer the X due to too low tuning strength, and end up with a flat line as estimate: then the RMS value will only depend on the variance of X. 
The R squared value would maybe be better, but if we average the RMS over seeds then we average over the variance of X path as well.

If X estimate is precisely zero we get an error in the determinant of Kx.
That is fixed by adding a small term.
But that problem could also be due to the constant dropping of sigma n without restoring it!
Needs a global unchangable startign sigma n and then we decrease it locally at every iteration.

Had to reset F estimate when we flip X estimate because I think it was trapped in a local minima leading to worse convergence for X

Re-converging after flipping gave worse RMSE values, so we're just taking the min RMSE vlaue of X and X flipped.
Though could it be the case that X converges better if it is flipped?
-- Now for ts 16 the after flip is always better

\subsection{29.05}
Discovered that: ts 16, good converge except for line stuck far down at end.
Attempts at fixing it: 
- re-adding random noise for initial guess:
Worked for that case.
No-flip RMSE: 0.3764127947575389

We are not spreading across enough of the domain.
- Try lowering the sigma_x from 100 to 50:
Did not work
- Try increasing the variance of generative (and inferring) X
Worked (obviously)

Having convergence issues at seed 9:
- that is right after plot figures warning.
- it is also a seed that gave us a very low variation in X except for a small number of sharp peaks.

######################################
Lambda strength: 16
Baseline expected spikes: 4.5399929762484854e-05
Expected no. of spikes at peak: 16
RMSE for X, Averaged across seeds: 0.2587602343114252
RMSE for X flipped: 1.221666999270192
RMSE for F, Averaged across seeds: 8.422289793214146
######################################

T = 100
lambda_strength and RMSE
16: 0.43703416548029617
12: 0.49388507368447404
10: 0.45196784300534765
8: 0.3528263031609709
6: 0.3867000231202763
4: 0.45159691185191636
2: 0.739551482437652
1: 

ts 2 has some unfortunate cases where it is upside down only in parts
Unfortunate seeds: 2, 14, (4 is unfortunate for 12), 

T = 10
lambda_strength and RMSE and R squared
Averaged over 12 seeds 
0.1 1.0364895956799882  -50.4728346460829
0.5 1.0406274710325414  -50.47464357537558
1   0.7422087009697954  -20.3087918773691
2   0.7466309328194045  -20.328251915084937
3   0.7472190837927086  -20.328615752135146
4   0.7527862724434419  -20.35709780525213
5   0.7404025520105525  -20.321719480758173
6   0.7350274848878883  -20.304027117377405
7   0.7760717833525844  -20.661516639707507
8   0.746736816844552   -20.409954797527746
9   0.7642916964361809  -20.61791412042584
10  0.7342163509093863  -20.44656243089726
11  0.6783056187618514  -20.138536419227183
12  0.6734560913326089  -20.129929968875064
13  0.6679172321859862  -20.115018289674417
14  0.6534704062828897  -20.081188426730588
15  0.6512345921874434  -20.238918624651173
16  0.6354140567049367  -20.044956895538427

T = 100

T = 1000
Ganske bra rmse for T = 1000! 
Kan godt kjøre en plotterunde.
Men vi trenger altså flere lave tuning strengths 


\subsection{02.05}
OpenMP driver og parallelliserer numpy-koden for å få brukt mer juice når jeg kjører ett skript (noe jeg har observert på min maskin).
Det mest effektive som Per Kristian foreslår er å begrense kjernebruken til 1 CPU per prosess, så bruken av hver CPU blir optimal, og så se hvor mange prosesser vi har plass til på siden av hverandre. 
Plan 1: 
Alternativ: Ikke begrens antall CPU-er i Open MP og kjør færre prosesser på siden av hverandre. 

\subsection{03.05}
export OMP_NUM_THREADS=1 setter 1 CPU på hver parallelle tråd. Antall tråder = len(seed).

Noticed difference in performance.
There's a fucking bug that makes the results bad. May come from adding function arguments to shit. 
Relax, we have different versions. We can handle this. 
- Attempt to highlight changes and see what is being done differently:

Checking how important initial X is:
L value for path varies due to different F!!!!!: 6287.955163043913, , 
seed 0       , offset 1.5: L -13565.381001321895 
seed original, offset 1.5: L -14806.454455027344    path                            sqrt(mean(X_estimate-previous_X))= 0.0017169788566815862
seed 2       , offset 1.5: L -13878.157314977567    path                            sqrt(mean(X_estimate-previous_X))= 0.0037116017844465625
seed 2       , offset 1.0: L -14801.72865808279     path                            sqrt(mean(X_estimate-previous_X))= 0.008494824335477224
seed 2       , offset 6.0: L  6249.93726898615      path  6287.955163043913         sqrt(mean(X_estimate-previous_X))= 0.04860226552057797 (This one flipped)
seed 2       , offset 5.0: L -13876.44994915217     path  -9755.027979408747        sqrt(mean(X_estimate-previous_X))= 0.048404832338805145 
seed 2       , offset 4.0: L -13876.44994915217     path  -17883.73500880844        sqrt(mean(X_estimate-previous_X))= 0.007112102245324013 
seed 0       , offset 4.0: L -15419.344845948024     path  -8550.152602300686        sqrt(mean(X_estimate-previous_X))= 0.002785984840746242 
frisluppen,    2pi random: L -14497.101410959049     path  -8550.152602300686        sqrt(mean(X_estimate-previous_X))= 0.007158166109343249 (FLIPPER IKKE!)
helt random start, path seed 12 i stedet for 11  DNF (Did not flip)
path seed 13 DNF
14 

Conclusion: Enough for a discussion, possibly a plot showing different results and having different L values given true F.


Changed on robust-efficient-script to include extra parameters. RMSE difference 0.08330439712884885 which is almost nothing. 
Must be something else. 

I suspect it may be due to the spread of tuning curves outside range of X when path does not spread.
For seed 16 path spread, ok inference.
Seed 

For good performance: 
Let the inducing grid stay in the range of the path! Huzzah! That is realistic anyway <3 
Actually quite important since we have to talk with the inducing points that are outside and also lose precision.

Now to decide whether to initialize X over the range of the entire X or just in the width of the path? Can do it in 0,2pi because we can. 
Even though we say that we know the X when we place the inducing points there. Those can be moved though. But is job.

############ Today's changes to parallel:
- change initialization to 2pi random random
- make x_grid_inducing range only over the path of X
- make x_grid_inducing a variable passed to function
- Kt still proiduced outside, x_grid_inducing and Kggplain inside.

